{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-11T17:30:37.398849Z",
     "iopub.status.busy": "2026-02-11T17:30:37.398579Z",
     "iopub.status.idle": "2026-02-11T17:30:37.712093Z",
     "shell.execute_reply": "2026-02-11T17:30:37.711393Z",
     "shell.execute_reply.started": "2026-02-11T17:30:37.398824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "!pip -q install nltk rouge-score gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:30:48.934495Z",
     "iopub.status.busy": "2026-02-11T17:30:48.934269Z",
     "iopub.status.idle": "2026-02-11T17:30:51.241070Z",
     "shell.execute_reply": "2026-02-11T17:30:51.240415Z",
     "shell.execute_reply.started": "2026-02-11T17:30:48.934472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction using the given code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:30:53.960970Z",
     "iopub.status.busy": "2026-02-11T17:30:53.960188Z",
     "iopub.status.idle": "2026-02-11T17:33:38.586596Z",
     "shell.execute_reply": "2026-02-11T17:33:38.585651Z",
     "shell.execute_reply.started": "2026-02-11T17:30:53.960934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found images at: /kaggle/input/datasets/adityajn105/flickr30k/Images\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 196MB/s] \n",
      "Extracting Features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 249/249 [01:58<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! 31783 images processed and saved to flickr30k_features.pkl\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, torch, torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_image_dir():\n",
    "    base_input = '/kaggle/input'\n",
    "    for root, dirs, files in os.walk(base_input):\n",
    "        if len([f for f in files if f.endswith('.jpg')]) > 1000:\n",
    "            return root\n",
    "    return None\n",
    "\n",
    "IMAGE_DIR = find_image_dir()\n",
    "OUTPUT_FILE = 'flickr30k_features.pkl'\n",
    "\n",
    "if IMAGE_DIR:\n",
    "    print(f\"Found images at: {IMAGE_DIR}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find the Flickr30k image directory. Please ensure the dataset is added.\")\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.img_names = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img), name\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model = nn.Sequential(*list(model.children())[:-1])  # Feature vector only\n",
    "model = nn.DataParallel(model).to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "dataset = FlickrDataset(IMAGE_DIR, transform)\n",
    "loader = DataLoader(dataset, batch_size=128, num_workers=4)\n",
    "\n",
    "features_dict = {}\n",
    "with torch.no_grad():\n",
    "    for imgs, names in tqdm(loader, desc=\"Extracting Features\"):\n",
    "        feats = model(imgs.to(device)).view(imgs.size(0), -1)\n",
    "        for i, name in enumerate(names):\n",
    "            features_dict[name] = feats[i].cpu().numpy()\n",
    "\n",
    "with open(OUTPUT_FILE, 'wb') as f:\n",
    "    pickle.dump(features_dict, f)\n",
    "\n",
    "print(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features generated successfully as *'flickr30k_features.pkl'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:33:44.564298Z",
     "iopub.status.busy": "2026-02-11T17:33:44.563619Z",
     "iopub.status.idle": "2026-02-11T17:33:44.738727Z",
     "shell.execute_reply": "2026-02-11T17:33:44.738087Z",
     "shell.execute_reply.started": "2026-02-11T17:33:44.564264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            image                                            caption\n",
       " 0  1000092795.jpg  Two young guys with shaggy hair look at their ...\n",
       " 1  1000092795.jpg  \" Two young , White males are outside near man...\n",
       " 2  1000092795.jpg   Two men in green shirts are standing in a yard .\n",
       " 3  1000092795.jpg       A man in a blue shirt standing in a garden .\n",
       " 4  1000092795.jpg            Two friends enjoy time spent together .,\n",
       " 158915)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "CAPTIONS_PATH = \"/kaggle/input/datasets/adityajn105/flickr30k/captions.txt\"\n",
    "# Loading captions into a dataframe/list of (image_name, caption) pairs\n",
    "rows = []\n",
    "with open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # Check for common formats (CSV or Tab-separated)\n",
    "        if \"\\t\" in line:\n",
    "            img, cap = line.split(\"\\t\", 1)\n",
    "        elif \",\" in line:\n",
    "            img, cap = line.split(\",\", 1)\n",
    "        else:\n",
    "            continue\n",
    "        rows.append((img.strip(), cap.strip()))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(rows, columns=[\"image\", \"caption\"])\n",
    "df.head(), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Captions\n",
    "### Removing unwanted characters and making lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:33:47.763493Z",
     "iopub.status.busy": "2026-02-11T17:33:47.763199Z",
     "iopub.status.idle": "2026-02-11T17:33:48.775105Z",
     "shell.execute_reply": "2026-02-11T17:33:48.774479Z",
     "shell.execute_reply.started": "2026-02-11T17:33:47.763469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>caption_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two young guys with shaggy hair look at their ...</td>\n",
       "      <td>two young guys with shaggy hair look at their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>\" Two young , White males are outside near man...</td>\n",
       "      <td>two young white males are outside near many bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "      <td>two men in green shirts are standing in a yard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "      <td>a man in a blue shirt standing in a garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "      <td>two friends enjoy time spent together</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image                                            caption  \\\n",
       "0  1000092795.jpg  Two young guys with shaggy hair look at their ...   \n",
       "1  1000092795.jpg  \" Two young , White males are outside near man...   \n",
       "2  1000092795.jpg   Two men in green shirts are standing in a yard .   \n",
       "3  1000092795.jpg       A man in a blue shirt standing in a garden .   \n",
       "4  1000092795.jpg            Two friends enjoy time spent together .   \n",
       "\n",
       "                                       caption_clean  \n",
       "0  two young guys with shaggy hair look at their ...  \n",
       "1  two young white males are outside near many bu...  \n",
       "2     two men in green shirts are standing in a yard  \n",
       "3         a man in a blue shirt standing in a garden  \n",
       "4              two friends enjoy time spent together  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean captions by removing unwanted characters and making lowercase\n",
    "def clean_caption(s: str) -> str:\n",
    "    s = s.lower()  # Convert to lowercase\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)  # Remove punctuation\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()  # Normalize spaces\n",
    "    return s\n",
    "\n",
    "df[\"caption_clean\"] = df[\"caption\"].apply(clean_caption)\n",
    "\n",
    "# View cleaned captions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Build Vocab & Convert 'Captions ‚Üí Token IDs'\n",
    "   ### 1. Building vocabulary by counting word frequencies\n",
    "   ### 2. Adding special tokens (<pad\\>, <unk\\>, <start\\>, <end\\>)\n",
    "   ### 3. Converting captions to token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:52:56.597891Z",
     "iopub.status.busy": "2026-02-11T17:52:56.597080Z",
     "iopub.status.idle": "2026-02-11T17:52:57.686081Z",
     "shell.execute_reply": "2026-02-11T17:52:57.685444Z",
     "shell.execute_reply.started": "2026-02-11T17:52:56.597862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sample caption: <start> two young guys with shaggy hair look at their hands while hanging out in the yard <end>\n",
      "<unk> ratio: 0.0%  (should be < 5-10%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption_clean</th>\n",
       "      <th>cap_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>two young guys with shaggy hair look at their ...</td>\n",
       "      <td>[2, 9348, 10000, 3933, 9884, 7663, 3949, 5093,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>two young white males are outside near many bu...</td>\n",
       "      <td>[2, 9348, 10000, 9801, 5195, 387, 5962, 5671, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>two men in green shirts are standing in a yard</td>\n",
       "      <td>[2, 9348, 5356, 4394, 3846, 7736, 387, 8366, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>a man in a blue shirt standing in a garden</td>\n",
       "      <td>[2, 71, 5203, 4394, 71, 956, 7733, 8366, 4394,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>two friends enjoy time spent together</td>\n",
       "      <td>[2, 9348, 3550, 2962, 9031, 1, 9063, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image                                      caption_clean  \\\n",
       "0  1000092795.jpg  two young guys with shaggy hair look at their ...   \n",
       "1  1000092795.jpg  two young white males are outside near many bu...   \n",
       "2  1000092795.jpg     two men in green shirts are standing in a yard   \n",
       "3  1000092795.jpg         a man in a blue shirt standing in a garden   \n",
       "4  1000092795.jpg              two friends enjoy time spent together   \n",
       "\n",
       "                                             cap_ids  \n",
       "0  [2, 9348, 10000, 3933, 9884, 7663, 3949, 5093,...  \n",
       "1  [2, 9348, 10000, 9801, 5195, 387, 5962, 5671, ...  \n",
       "2  [2, 9348, 5356, 4394, 3846, 7736, 387, 8366, 4...  \n",
       "3  [2, 71, 5203, 4394, 71, 956, 7733, 8366, 4394,...  \n",
       "4            [2, 9348, 3550, 2962, 9031, 1, 9063, 3]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Special tokens\n",
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<start>\", \"<end>\"]\n",
    "PAD, UNK, START, END = SPECIALS\n",
    "\n",
    "# Count word frequencies\n",
    "min_freq = 3  # You can change this if you want more/less common words\n",
    "\n",
    "counter = Counter()\n",
    "for cap in df[\"caption_clean\"]:\n",
    "    counter.update(cap.split())\n",
    "\n",
    "# Filter words by frequency\n",
    "vocab_words = [w for w, c in counter.items() if c >= min_freq]\n",
    "vocab = SPECIALS + sorted(vocab_words)\n",
    "\n",
    "# Create word-to-id and id-to-word mappings\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Get special token IDs\n",
    "pad_id = word2idx[PAD]\n",
    "unk_id = word2idx[UNK]\n",
    "start_id = word2idx[START]\n",
    "end_id = word2idx[END]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Function to convert captions to token IDs\n",
    "def encode_caption(cap: str):\n",
    "    tokens = cap.split()\n",
    "    ids = [start_id] + [word2idx.get(t, unk_id) for t in tokens] + [end_id]\n",
    "    return ids\n",
    "\n",
    "# Convert captions to token IDs\n",
    "df[\"cap_ids\"] = df[\"caption_clean\"].apply(encode_caption)\n",
    "\n",
    "# Safety check - should NOT be mostly <unk>\n",
    "sample_cap = df[\"cap_ids\"].iloc[0]\n",
    "decoded_sample = \" \".join(idx2word.get(i, \"<unk>\") for i in sample_cap)\n",
    "print(\"Decoded sample caption:\", decoded_sample)\n",
    "unk_count = decoded_sample.count(\"<unk>\")\n",
    "total_words = len(sample_cap)\n",
    "print(f\"<unk> ratio: {unk_count / total_words:.1%}  (should be < 5-10%)\")\n",
    "\n",
    "# View the result\n",
    "df[[\"image\", \"caption_clean\", \"cap_ids\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:53:03.464703Z",
     "iopub.status.busy": "2026-02-11T17:53:03.464100Z",
     "iopub.status.idle": "2026-02-11T17:53:03.468630Z",
     "shell.execute_reply": "2026-02-11T17:53:03.467940Z",
     "shell.execute_reply.started": "2026-02-11T17:53:03.464659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "<start>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(idx2word[2])\n",
    "print(idx2word[start_id])\n",
    "print(start_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:53:07.339125Z",
     "iopub.status.busy": "2026-02-11T17:53:07.338521Z",
     "iopub.status.idle": "2026-02-11T17:53:07.343682Z",
     "shell.execute_reply": "2026-02-11T17:53:07.342958Z",
     "shell.execute_reply.started": "2026-02-11T17:53:07.339094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10029\n",
      "Sample words and ids:\n",
      "<start>    ‚Üí 2\n",
      "<end>      ‚Üí 3\n",
      "a          ‚Üí 71\n",
      "the        ‚Üí 8940\n",
      "in         ‚Üí 4394\n",
      "man        ‚Üí 5203\n",
      "dog        ‚Üí 2640\n",
      "two        ‚Üí 9348\n",
      "young      ‚Üí 10000\n",
      "shaggy     ‚Üí 7663\n"
     ]
    }
   ],
   "source": [
    "# After: word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "print(\"Vocab size:\", len(word2idx))\n",
    "print(\"Sample words and ids:\")\n",
    "for w in [\"<start>\", \"<end>\", \"a\", \"the\", \"in\", \"man\", \"dog\", \"two\", \"young\", \"shaggy\"]:\n",
    "    print(f\"{w:10} ‚Üí {word2idx.get(w, 'MISSING')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing *Max Length* + *Padding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:53:09.862113Z",
     "iopub.status.busy": "2026-02-11T17:53:09.861549Z",
     "iopub.status.idle": "2026-02-11T17:53:10.339729Z",
     "shell.execute_reply": "2026-02-11T17:53:10.338975Z",
     "shell.execute_reply.started": "2026-02-11T17:53:09.862082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen max_len: 24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = df[\"cap_ids\"].apply(len).values\n",
    "max_len = int(np.percentile(lengths, 95))  # stable choice\n",
    "max_len = max(max_len, 10)\n",
    "print(\"Chosen max_len:\", max_len)\n",
    "\n",
    "def pad_to_max(ids, max_len):\n",
    "    ids = ids[:max_len]\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [pad_id] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "df[\"cap_ids_pad\"] = df[\"cap_ids\"].apply(lambda x: pad_to_max(x, max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into (train/val/test) by image \n",
    "### To avoid leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:53:12.541060Z",
     "iopub.status.busy": "2026-02-11T17:53:12.540295Z",
     "iopub.status.idle": "2026-02-11T17:53:12.707459Z",
     "shell.execute_reply": "2026-02-11T17:53:12.706743Z",
     "shell.execute_reply.started": "2026-02-11T17:53:12.541027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128715, 14305, 15895)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "unique_images = df[\"image\"].unique()\n",
    "\n",
    "train_imgs, test_imgs = train_test_split(unique_images, test_size=0.1, random_state=42)\n",
    "train_imgs, val_imgs  = train_test_split(train_imgs, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df = df[df[\"image\"].isin(train_imgs)].reset_index(drop=True)\n",
    "val_df   = df[df[\"image\"].isin(val_imgs)].reset_index(drop=True)\n",
    "test_df  = df[df[\"image\"].isin(test_imgs)].reset_index(drop=True)\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset/DataLoader using cached *.pkl* features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:53:14.946452Z",
     "iopub.status.busy": "2026-02-11T17:53:14.945641Z",
     "iopub.status.idle": "2026-02-11T17:53:15.232350Z",
     "shell.execute_reply": "2026-02-11T17:53:15.231614Z",
     "shell.execute_reply.started": "2026-02-11T17:53:14.946416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "with open(\"flickr30k_features.pkl\", \"rb\") as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = row[\"image\"]\n",
    "        cap = torch.tensor(row[\"cap_ids_pad\"], dtype=torch.long)\n",
    "\n",
    "        # inputs and targets (teacher forcing)\n",
    "        cap_in  = cap[:-1]\n",
    "        cap_out = cap[1:]\n",
    "\n",
    "        img_feat = torch.tensor(features[img], dtype=torch.float32)  # (2048,)\n",
    "        return img_feat, cap_in, cap_out, img\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(CaptionDataset(train_df), batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(CaptionDataset(val_df), batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(CaptionDataset(test_df), batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Seq2Seq ‚áí **Encoder:** *Linear* + **Decoder:** *LSTM/GRU*\n",
    "### **Encoder:** Linear 2048 ‚Üí hidden_size\n",
    "### **Decoder:** LSTM/GRU with embeddings ‚á¢ init hidden from encoder ‚á¢ output linear to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:16:32.769657Z",
     "iopub.status.busy": "2026-02-11T18:16:32.769335Z",
     "iopub.status.idle": "2026-02-11T18:16:32.872684Z",
     "shell.execute_reply": "2026-02-11T18:16:32.871858Z",
     "shell.execute_reply.started": "2026-02-11T18:16:32.769629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(2048, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)  # added to avoid overfitting\n",
    "\n",
    "    def forward(self, img_feat):\n",
    "        # img_feat: (B, 2048)  ‚Üê image features from ResNet\n",
    "        h0 = torch.tanh(self.fc(img_feat))  # (B, hidden_size)\n",
    "        h0 = self.dropout(h0)  # added to avoid overfitting\n",
    "        return h0.unsqueeze(0)  # ‚Üê now (1, B, hidden_size) Nano Improved\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_size=512, rnn_type=\"lstm\"):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type.lower()\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.embed_dropout = nn.Dropout(0.3)  # added to avoid overfitting\n",
    "\n",
    "        if self.rnn_type == \"gru\":\n",
    "            self.rnn = nn.GRU(\n",
    "                embed_dim,\n",
    "                hidden_size,\n",
    "                batch_first=True,\n",
    "                dropout=0.3  # added to avoid overfitting (active if num_layers > 1)\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(\n",
    "                embed_dim,\n",
    "                hidden_size,\n",
    "                batch_first=True,\n",
    "                dropout=0.3  # added to avoid overfitting (active if num_layers > 1)\n",
    "            )\n",
    "\n",
    "        self.output_dropout = nn.Dropout(0.4)  # added to avoid overfitting\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, cap_in, h0):\n",
    "        # cap_in: (B, seq_len)\n",
    "        # h0: (1, B, hidden_size)   ‚Üê we now receive this shape (Nano Improved)\n",
    "\n",
    "        x = self.embed(cap_in)  # (B, seq_len, embed_dim)\n",
    "        x = self.embed_dropout(x)  # added to avoid overfitting\n",
    "\n",
    "        #  Nano Improved removed .unsqueeze(0)\n",
    "        if self.rnn_type == \"gru\":\n",
    "            # GRU needs (num_layers, B, hidden)\n",
    "            out, hn = self.rnn(x, h0)  \n",
    "        else:\n",
    "            # LSTM needs (h0, c0)\n",
    "            c0 = torch.zeros_like(h0)  # (1, B, hidden_size)\n",
    "            out, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "\n",
    "        # out: (B, seq_len, hidden_size)\n",
    "        out = self.output_dropout(out)  # added to avoid overfitting\n",
    "\n",
    "        logits = self.fc_out(out)  # (B, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_size=512, rnn_type=\"lstm\"):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(hidden_size=hidden_size)\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            rnn_type=rnn_type\n",
    "        )\n",
    "\n",
    "    def forward(self, img_feat, cap_in):\n",
    "        # img_feat: (B, 2048)\n",
    "        # cap_in:   (B, seq_len)\n",
    "\n",
    "        h0 = self.encoder(img_feat)       # now returns (1, B, H) Nano Improved\n",
    "        logits = self.decoder(cap_in, h0) # (B, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=256,\n",
    "    hidden_size=512,\n",
    "    rnn_type=\"lstm\"\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:53:39.568517Z",
     "iopub.status.busy": "2026-02-11T17:53:39.568150Z",
     "iopub.status.idle": "2026-02-11T17:53:39.573700Z",
     "shell.execute_reply": "2026-02-11T17:53:39.572922Z",
     "shell.execute_reply.started": "2026-02-11T17:53:39.568483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Pick one row from df\n",
    "# example_row = df.iloc[0]\n",
    "# print(\"Original clean caption:\", example_row[\"caption_clean\"])\n",
    "# print(\"Encoded ids     :\", example_row[\"cap_ids\"])\n",
    "# print(\"Padded ids      :\", example_row[\"cap_ids_pad\"])\n",
    "# print(\"Decoded back    :\", \" \".join(idx2word.get(i, \"<unk>\") for i in example_row[\"cap_ids_pad\"] if i != pad_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:25:45.824311Z",
     "iopub.status.busy": "2026-02-11T17:25:45.823423Z",
     "iopub.status.idle": "2026-02-11T17:25:45.827556Z",
     "shell.execute_reply": "2026-02-11T17:25:45.826844Z",
     "shell.execute_reply.started": "2026-02-11T17:25:45.824278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Vocab size:\", vocab_size)\n",
    "# print(\"Most common words:\", counter.most_common(20))\n",
    "\n",
    "# # Check if <start>, <end>, <pad>, <unk> are present\n",
    "# print(\"Special tokens:\", [w for w in SPECIALS if w in word2idx])\n",
    "\n",
    "# # See if very common words like \"a\", \"the\", \"in\", \"on\" are in vocab\n",
    "# common = [\"a\", \"the\", \"in\", \"on\", \"is\", \"with\", \"and\", \"man\", \"woman\", \"dog\"]\n",
    "# for w in common:\n",
    "#     print(f\"{w:8} ‚Üí id {word2idx.get(w, 'NOT IN VOCAB')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T17:25:50.638397Z",
     "iopub.status.busy": "2026-02-11T17:25:50.638100Z",
     "iopub.status.idle": "2026-02-11T17:25:50.641985Z",
     "shell.execute_reply": "2026-02-11T17:25:50.641409Z",
     "shell.execute_reply.started": "2026-02-11T17:25:50.638368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for i, (img_feat, cap_in, cap_out, _) in enumerate(train_loader):\n",
    "#         if i >= 3: break  # just first few batches\n",
    "#         img_feat = img_feat.to(device)\n",
    "#         cap_in  = cap_in.to(device)\n",
    "#         cap_out = cap_out.to(device)\n",
    "        \n",
    "#         logits = model(img_feat, cap_in)           # (B, T, V)\n",
    "#         pred_ids = torch.argmax(logits, dim=-1)    # (B, T)\n",
    "        \n",
    "#         print(\"\\nBatch\", i)\n",
    "#         for b in range(min(4, pred_ids.size(0))):  # show 4 examples\n",
    "#             p = pred_ids[b].cpu().tolist()\n",
    "#             predicted_words = [idx2word.get(tid, \"<unk>\") for tid in p if tid != pad_id]\n",
    "#             target_words   = [idx2word.get(tid, \"<unk>\") for tid in cap_out[b].cpu().tolist() if tid != pad_id]\n",
    "            \n",
    "#             print(f\"Predicted: {' '.join(predicted_words[:30])}\")\n",
    "#             print(f\"Target   : {' '.join(target_words[:30])}\")\n",
    "#             print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with CrossEntropy + Adam\n",
    "### 1. Ignoring pad in CrossEntropy as asked in the assignment\n",
    "### 2. Using Adam Opitmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:16:52.457600Z",
     "iopub.status.busy": "2026-02-11T18:16:52.457300Z",
     "iopub.status.idle": "2026-02-11T18:32:32.395041Z",
     "shell.execute_reply": "2026-02-11T18:32:32.394150Z",
     "shell.execute_reply.started": "2026-02-11T18:16:52.457573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | train_loss=4.7613 | val_loss=4.1434\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | train_loss=4.0276 | val_loss=3.7737\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | train_loss=3.7304 | val_loss=3.5515\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | train_loss=3.5453 | val_loss=3.4192\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | train_loss=3.4190 | val_loss=3.3291\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | train_loss=3.3253 | val_loss=3.2631\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | train_loss=3.2512 | val_loss=3.2093\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | train_loss=3.1904 | val_loss=3.1717\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | train_loss=3.1400 | val_loss=3.1368\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | train_loss=3.0976 | val_loss=3.1112\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | train_loss=3.0593 | val_loss=3.0876\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | train_loss=3.0251 | val_loss=3.0683\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | train_loss=2.9944 | val_loss=3.0499\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | train_loss=2.9667 | val_loss=3.0370\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | train_loss=2.9427 | val_loss=3.0247\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | train_loss=2.9197 | val_loss=3.0121\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | train_loss=2.8979 | val_loss=3.0045\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | train_loss=2.8791 | val_loss=2.9963\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | train_loss=2.8610 | val_loss=2.9868\n",
      "‚úÖ Validation improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | train_loss=2.8429 | val_loss=2.9791\n",
      "‚úÖ Validation improved. Model saved.\n",
      "Training finished. Best model saved as best_model.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=2e-4,              # nano improved (slightly reduced LR)\n",
    "    weight_decay=1e-5     # added to avoid overfitting (L2 regularization)\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    'min',\n",
    "    factor=0.5,\n",
    "    patience=4\n",
    ")  # nano improved\n",
    "\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for img_feat, cap_in, cap_out, _ in tqdm(loader, leave=False):\n",
    "        # img_feat: (B, 2048)\n",
    "        # cap_in:   (B, T)\n",
    "        # cap_out:  (B, T)\n",
    "\n",
    "        img_feat = img_feat.to(device)\n",
    "        cap_in   = cap_in.to(device)\n",
    "        cap_out  = cap_out.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(img_feat, cap_in)  # (B, T, V)\n",
    "\n",
    "            B, T, V = logits.shape  # Batch, Time, Vocab\n",
    "            loss = criterion(\n",
    "                logits.reshape(B*T, V), \n",
    "                cap_out.reshape(B*T)\n",
    "            )\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(),\n",
    "                    max_norm=5.0\n",
    "                )  # nano improved (gradient clipping prevents exploding gradients)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return float(np.mean(losses))\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val = 1e9\n",
    "\n",
    "# -------------------------\n",
    "# üî• Early Stopping Setup\n",
    "# -------------------------\n",
    "early_stop_patience = 6   # added to avoid overfitting\n",
    "no_improve_epochs = 0     # nano improved\n",
    "\n",
    "epochs = 20  # nano improved\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "\n",
    "    tr = run_epoch(train_loader, train=True)\n",
    "    va = run_epoch(val_loader, train=False)\n",
    "\n",
    "    scheduler.step(va)   # nano improved (adaptive LR)\n",
    "\n",
    "    train_losses.append(tr)\n",
    "    val_losses.append(va)\n",
    "\n",
    "    print(f\"Epoch {ep}/{epochs} | train_loss={tr:.4f} | val_loss={va:.4f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # ‚úÖ Check improvement\n",
    "    # -------------------------\n",
    "    if va < best_val - 1e-4:  # added to avoid overfitting (ignore tiny fluctuations)\n",
    "        best_val = va\n",
    "        no_improve_epochs = 0   # nano improved (reset counter)\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"‚úÖ Validation improved. Model saved.\")\n",
    "    else:\n",
    "        no_improve_epochs += 1  # nano improved\n",
    "        print(f\"No improvement for {no_improve_epochs} epoch(s).\")\n",
    "\n",
    "    # -------------------------\n",
    "    # üõë Early Stopping Condition\n",
    "    # -------------------------\n",
    "    if no_improve_epochs >= early_stop_patience:\n",
    "        print(\"üõë Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished. Best model saved as best_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting lost curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:32:40.784647Z",
     "iopub.status.busy": "2026-02-11T18:32:40.783732Z",
     "iopub.status.idle": "2026-02-11T18:32:40.910565Z",
     "shell.execute_reply": "2026-02-11T18:32:40.909971Z",
     "shell.execute_reply.started": "2026-02-11T18:32:40.784612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference ‚Üí Greedy Search + Beam Search\n",
    "## Greedy Search\n",
    "### ‚û• selects the most likely next word in each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:32:45.657595Z",
     "iopub.status.busy": "2026-02-11T18:32:45.657307Z",
     "iopub.status.idle": "2026-02-11T18:32:45.663896Z",
     "shell.execute_reply": "2026-02-11T18:32:45.663366Z",
     "shell.execute_reply.started": "2026-02-11T18:32:45.657568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(img_feat_2048, max_len=30):\n",
    "    # Move image feature to the device\n",
    "    img_feat = torch.tensor(img_feat_2048, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 2048)\n",
    "    \n",
    "    # Get the initial hidden state from the encoder\n",
    "    h0 = model.encoder(img_feat)  # now (1, 1, hidden)\n",
    "\n",
    "    # Start token (used as the first word in the sequence)\n",
    "    cur = torch.tensor([[start_id]], dtype=torch.long).to(device)  # (1, 1)\n",
    "    out_tokens = []\n",
    "\n",
    "    # Generate tokens up to max_len or until <end> token is produced\n",
    "    for _ in range(max_len):\n",
    "        logits = model.decoder(cur, h0)  # (1, T, V) where T is time steps and V is vocabulary size\n",
    "        next_logits = logits[:, -1, :]       # (1, vocab) nano improved\n",
    "        next_id = torch.argmax(next_logits, dim=-1).item()  # Get the next token ID (argmax)\n",
    "        \n",
    "        if next_id == end_id:  # Stop if <end> token is predicted\n",
    "            break\n",
    "        \n",
    "        out_tokens.append(next_id)\n",
    "        cur = torch.cat([cur, torch.tensor([[next_id]], device=device)], dim=1)  # Update input with the predicted token\n",
    "\n",
    "    # Convert token IDs to words || Nano improved UNK\n",
    "    words = [idx2word.get(i, \"<unk>\") for i in out_tokens]\n",
    "    return \" \".join(words)  # Return the generated caption as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "### ‚û• maintains multiple hypotheses (sequences) at each step and selects the top-k most probable sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:32:48.095081Z",
     "iopub.status.busy": "2026-02-11T18:32:48.094576Z",
     "iopub.status.idle": "2026-02-11T18:32:48.104229Z",
     "shell.execute_reply": "2026-02-11T18:32:48.103505Z",
     "shell.execute_reply.started": "2026-02-11T18:32:48.095053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_decode(img_feat_2048, beam_width=3, max_len=30):\n",
    "    # Move image feature to the device\n",
    "    img_feat = torch.tensor(img_feat_2048, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 2048)\n",
    "    \n",
    "    # Get the initial hidden state from the encoder\n",
    "    h_init = model.encoder(img_feat)  # (1, hidden_size) # Nano now (1,1,H)  \n",
    "    # Nano improved this part (renamed h0 ‚Üí h_init to support per-beam hidden tracking)\n",
    "\n",
    "    # Initialize the beam with the start token and score 0\n",
    "    # beam now stores (sequence, score, hidden_state)\n",
    "    beams = [([start_id], 0.0, h_init)]  \n",
    "    # Nano improved this part (added hidden state inside beam tuple)\n",
    "\n",
    "    finished = []\n",
    "\n",
    "    # Generate tokens up to max_len or until <end> token is produced\n",
    "    for step in range(max_len):  \n",
    "        # Nano improved this part (added step variable for clarity/debugging)\n",
    "        \n",
    "        new_beams = []\n",
    "        \n",
    "        # Expand each beam\n",
    "        for seq, score, hidden in beams:  \n",
    "            # Nano improved this part (now unpack hidden state per beam)\n",
    "\n",
    "            if seq[-1] == end_id:  # If <end> token is generated, add to finished list\n",
    "                finished.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            cur = torch.tensor([seq], dtype=torch.long).to(device)  # Current sequence (input to decoder)\n",
    "\n",
    "            # logits = model.decoder(cur, h0)  # (1, T, V)  # Old version\n",
    "            logits = model.decoder(cur, hidden)[:, -1, :]  \n",
    "            # Nano improved this part (use per-beam hidden and directly select last timestep)  # (1, V)\n",
    "\n",
    "            # log_probs = torch.log_softmax(logits[0, -1], dim=-1)  # Old version\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)  \n",
    "            # Nano improved this part (logits already sliced to last timestep)\n",
    "\n",
    "            topk = torch.topk(log_probs, beam_width)  # Get top-k most probable tokens\n",
    "            \n",
    "            for i in range(beam_width):  \n",
    "                # Nano improved this part (dimension indexing updated)\n",
    "                \n",
    "                next_id = topk.indices[0, i].item()  \n",
    "                next_logp = topk.values[0, i].item()  \n",
    "                # Nano improved this part (correct indexing for 2D tensor)\n",
    "\n",
    "                new_seq = seq + [next_id]\n",
    "                new_score = score + next_logp\n",
    "\n",
    "                # For simplicity, reuse same hidden (not perfect, but works better than before)\n",
    "                new_beams.append((new_seq, new_score, hidden))  \n",
    "                # Nano improved this part (hidden state carried per beam)\n",
    "\n",
    "        # Sort and keep only the top-k beams\n",
    "        new_beams.sort(key=lambda x: x[1], reverse=True)  # Sort by score (probability)\n",
    "        beams = new_beams[:beam_width]  # Keep only the top-k beams\n",
    "\n",
    "    # Add finished beams to the list\n",
    "    finished.extend(beams)\n",
    "    finished.sort(key=lambda x: x[1], reverse=True)  # Sort finished beams by score\n",
    "\n",
    "    # Take the best beam (highest score)\n",
    "    best_seq = finished[0][0]\n",
    "\n",
    "    # Remove <start> and <end> tokens and convert to words\n",
    "    best_seq = [t for t in best_seq if t not in (start_id, end_id, pad_id)]\n",
    "    \n",
    "    # words = [idx2word.get(i, UNK) for i in best_seq]  # Old version\n",
    "    words = [idx2word.get(i, \"<unk>\") for i in best_seq]  \n",
    "    # Nano improved this part (standardized unknown token to \"<unk>\")\n",
    "\n",
    "    return \" \".join(words)  # Return the best generated caption as a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For BLEU-4 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:07:48.514965Z",
     "iopub.status.busy": "2026-02-11T18:07:48.514656Z",
     "iopub.status.idle": "2026-02-11T18:08:38.706922Z",
     "shell.execute_reply": "2026-02-11T18:08:38.706203Z",
     "shell.execute_reply.started": "2026-02-11T18:07:48.514939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:50<00:00, 19.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 (beam=3): 0.17146549610441278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "def compute_bleu(test_df, features, decode_func=beam_decode, beam_width=3, num_samples=1000):\n",
    "    bleu_scores = []\n",
    "    sampled_images = random.sample(list(test_df[\"image\"].unique()), min(num_samples, len(test_df[\"image\"].unique())))\n",
    "    \n",
    "    for img_name in tqdm(sampled_images):\n",
    "        gts = test_df[test_df[\"image\"] == img_name][\"caption_clean\"].str.split().tolist()\n",
    "        pred_str = decode_func(features[img_name], beam_width=beam_width)\n",
    "        pred = pred_str.split()\n",
    "        \n",
    "        if len(pred) == 0:\n",
    "            continue\n",
    "            \n",
    "        score = sentence_bleu(gts, pred, weights=(0.25,0.25,0.25,0.25),\n",
    "                              smoothing_function=chencherry.method1)\n",
    "        bleu_scores.append(score)\n",
    "    \n",
    "    return np.mean(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "# Run after training (example)\n",
    "print(\"BLEU-4 (beam=3):\", compute_bleu(test_df, features, beam_decode, beam_width=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T19:30:40.029076Z",
     "iopub.status.busy": "2026-02-11T19:30:40.028758Z",
     "iopub.status.idle": "2026-02-11T19:30:40.047304Z",
     "shell.execute_reply": "2026-02-11T19:30:40.046160Z",
     "shell.execute_reply.started": "2026-02-11T19:30:40.029041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/1112798836.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Select 5 random images from the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msample_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Select 5 random images from the test set\n",
    "sample_images = random.sample(list(test_imgs), 5)\n",
    "\n",
    "for img_name in sample_images:\n",
    "    # Load image\n",
    "    img_path = os.path.join(IMAGE_DIR, img_name)  # Update with correct image folder path\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    # Ground truth caption(s)\n",
    "    gt_row = test_df[test_df[\"image\"] == img_name].iloc[0]\n",
    "    gt_caption = gt_row[\"caption_clean\"]\n",
    "    \n",
    "    # Generate captions using Greedy and Beam Search\n",
    "    pred_greedy = greedy_decode(features[img_name])\n",
    "    pred_beam = beam_decode(features[img_name], beam_width=3)\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Display the captions\n",
    "    print(f\"IMAGE: {img_name}\")\n",
    "    print(f\"Ground Truth Caption: {gt_caption}\")\n",
    "    print(f\"Predicted Caption (Greedy): {pred_greedy}\")\n",
    "    print(f\"Predicted Caption (Beam): {pred_beam}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:10:16.850109Z",
     "iopub.status.busy": "2026-02-11T18:10:16.849701Z",
     "iopub.status.idle": "2026-02-11T18:10:16.854422Z",
     "shell.execute_reply": "2026-02-11T18:10:16.853817Z",
     "shell.execute_reply.started": "2026-02-11T18:10:16.850082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10029\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1. Save vocabulary (already built earlier ‚Äî save as pickle)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import pickle\n",
    "\n",
    "vocab_data = {\n",
    "    'word2idx': word2idx,\n",
    "    'idx2word': idx2word,\n",
    "    'vocab_size': vocab_size,\n",
    "    'pad_id': pad_id,\n",
    "    'unk_id': unk_id,\n",
    "    'start_id': start_id,\n",
    "    'end_id': end_id,\n",
    "    'min_freq': min_freq,           # just for info\n",
    "}\n",
    "\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_data, f)\n",
    "\n",
    "print(\"Saved: vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Change filename as needed\n",
    "FileLink(r'best_model.pt')\n",
    "FileLink(r'vocab.pkl')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 623329,
     "sourceId": 1111749,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
